===========================================================================
------------------------------- INSTRUCTIONS ------------------------------
===========================================================================

>> CODE STRUCTURE <<

Launch 'INSTALL' to add the path of all folders:
- 'Algs' contains all the algorithms.
- 'Library' contains all the policies, some functions for sampling and 
   other learning stuff.
- 'MO_Library' contains functions used in the Multi-Objective framework.
- 'Utilities' contains functions of general use.
- Other folders contain the models and the simulators of the MPDs.


>> HOW TO ADD A NEW MDP <<

Each MDP requires some mandatory functions:

- NAME_mdpvariables : contains the details of the problem (number of state 
                      variables, actions, rewards, discount factor, ...),
- NAME_simulator    : defines the reward and transition function.

Additionally, there are some functions used to better organize the code:

- NAME_environment  : has additional details of the problem environment,
- NAME_basis        : defines the features used to represent a state.

If any additional MDP is added to the framework, only three functions have 
to be modified: 'settings', 'settings_episodic' and 'getReferenceFront'.

Function 'settings' defines the learning setup, i.e., the policy to use and
the number of episodes and steps used to evaluate it.

Function 'settings_episodic' is similar to the previous one, but for 
episodic algorithms. It defines the initial mean and covariance for the 
distribution used to sample the parameters of a policy (defined in 
'settings'). The distribution is then initialized by the algorithm used.

Function 'getReferenceFront' returns all the details related to the multi-
objective setup, i.e., the reference frontier used for comparison, the 
utopia and antiutopia point.




===========================================================================
---------------------------------- NOTES ----------------------------------
===========================================================================

With episode-based algorithms (e.g. REPS, PGPE, NES) it is recommended to 
make the lower-level policy deterministic (use 'policy.makeDeterministic').
The reason is that such algorithms cannot deal with high stochasticity, as 
each parameter vector is evaluated only on one episode and therefore its
estimate would be inaccurate, unless we use a lot of samples to learn. 
Also, for Gaussian with full covariance (without Cholesky decomposition) 
the covariance matrix generated by the upper-level policy could be 
not positive-semidefinite (SPD).

Anyway, they can manage low stochasticity and if you want your low-level 
policy to be stochastic you can of course do it.
___________________________________________________________________________

In episodic policy search (e.g. REPS, NES) you can use samples from the 
last N_MAX policies to stabilize the policy update. This stabilization is 
important to keep the shape of the explorative variance of the policy in 
high dimensional action spaces. However, it also slows down the convergence 
speed (the KL divergence will be 0 only if the last N_MAX samples are 
optimal, i.e., if the variance of the final policy is very low).
___________________________________________________________________________

In PFA and RA, while checking for the minimal-norm Pareto-ascent direction, 
the gradients are always normalized (i.e., we normalize the gradients even 
if 'norm(grad_i) < 1'). If the gradients are not normalized, then the 
minimial-norm Pareto-ascent direction is expected to be mostly influenced 
by the elements which have smaller norms. Since gradients with small norm 
are usually associated with objectives that have already achieved a fair 
degree of convergence, the utility of considering these directions for a 
well-balanced correction is questionable. 
This observation points out the necessity of normalizing ALL the gradients 
when looking for the minimal-norm Pareto-ascent direction (Desideri, 
Multiple-gradient descent algorithm for multiobjective optimization, 2012).
___________________________________________________________________________

PFA (usually) needs a randomization of the policy after the optimization of 
one objective. Such randomization is needed to guarantee enough exploration 
to optimize the remaining objectives and depends on the policy used. 

For instance: 
- for a Gibbs policy we can reduce the temperature or scale theta with a 
  constant factor (e.g., we can halve theta),
- for a Gaussian policy we can reset the covariance matrix at its initial 
  value or we can, again, scale it.

Using the entropy as a scaling factor seems the most reasonable choice, but 
for Gaussians the (differential) entropy can be negative, while for Gibbs 
policies it could be too low.
___________________________________________________________________________

When evaluating a policy with 'evaluate_policies', you can choose to make 
it deterministic or not. In MORL, this affects the resulting Pareto 
frontier, as any convex combination of the points one the deterministic 
frontier belongs to the stochastic frontier.
Of course, considering only deterministic policies is much less time 
consuming, as only 1 episode is required to evaluate it (except when the
environment is stochastic).
