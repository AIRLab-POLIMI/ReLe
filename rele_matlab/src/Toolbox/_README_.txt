%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
------------------------------- INSTRUCTIONS ------------------------------
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Launch 'INSTALL' to add the paths of all folders.
___________________________________________________________________________

'Algs' contains all the algorithms.

'Library' contains all the policies, some functions for sampling and other 
learning stuff.

'MO_library' contains functions used in the Multi-Objective framework.

Other folders contain the models and the simulators of the MPDs.

Other functions not in any folder are of general use.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
---------------------------------- NOTES ----------------------------------
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

When using episode-based algorithms (e.g. REPS, PGPE, NES) you need to make 
the lower-level policy deterministic (use 'policy.makeDeterministic').
The reason is that such algorithms cannot deal with high stochasticity and 
because the covariance matrix generated by the upper-level policy could not 
positive-semidefinite (SPD).

More precisely:
- for a Gibbs policy there is no variance (so there is no 'SPD problem') 
  but you should still make it deterministic;
- for a Gaussian policy there is the 'SPD problem' except for the logistics
  and for the ones with non-learnable covariance.

Anyway, they can manage low stochasticity and if you want your low-level 
policy to be stochastic you can of course do it.
___________________________________________________________________________

In episodic policy search (e.g. REPS, NES) you can use samples from the 
last N_MAX policies to stabilize the policy update. This stabilization is 
important to keep the shape of the explorative variance of the policy in 
high dimensional action spaces. However, it also slows down the convergence 
speed (the KL divergence will be 0 only if the last N_MAX samples are 
optimal, i.e., if the variance of the final policy is very low).
___________________________________________________________________________

In PFA and RA, while checking for the minimal-norm Pareto-ascent direction, 
the gradients are always normalized (i.e., we normalize the gradients even 
if 'norm(grad_i) < 1'). If the gradients are not normalized, then the 
minimial-norm Pareto-ascent direction is expected to be mostly influenced 
by the elements which have smaller norms. Since gradients with small norm 
are usually associated with objectives that have already achieved a fair 
degree of convergence, the utility of considering these directions for a 
well-balanced correction is questionable. 
This observation points out the necessity of normalizing ALL the gradients 
when looking for the minimal-norm Pareto-ascent direction (Desideri, 
Multiple-gradient descent algorithm for multiobjective optimization, 2012).
___________________________________________________________________________

PFA (usually) needs a randomization of the policy after the optimization of 
one objective. Such randomization is needed to guarantee enough exploration 
to optimize the remaining objectives and depends on the policy used. 

For instance: 
- for a Gibbs policy we can reduce the temperature or scale theta with a 
  constant factor (for instance we can halve theta),
- for a Gaussian policy we can reset the covariance matrix at its initial 
  value or we can, again, scale it.

Using the entropy as a scaling factor seems the most reasonable choice, but 
for Gaussians the (differential) entropy can be negative, while for Gibbs 
policies it could be too low.
For these reasons, the randomization is not integrated in the code and you 
need to choose yours according to the policy and to the MPD.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
