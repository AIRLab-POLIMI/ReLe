%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
------------------------------- INSTRUCTIONS ------------------------------
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Launch 'install.m' to add the paths of all folders.
___________________________________________________________________________

'Algs' contains all the algorithms.

'Library' contains all the policies, some functions for sampling and other 
learning stuff.

'MO_library' contains functions used in the Multi-Objective framework.

Other folders contain the models and the simulators of the MPDs.

Other functions not in any folder are of general use.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
---------------------------------- NOTES ----------------------------------
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

When using episode-based algorithms (e.g. REPS, PGPE, NES) you need to set 
the number of episodes to 1 (use 'settings.m') and to make the lower-level 
policy deterministic (use 'policy.makeDeterministic').
This becuase such algorithms cannot deal with high stochasticity and 
because the covariance matrix generated by the upper-level policy could not 
positive-semidefinite (SPD).

More precisely:
- for a Gibbs policy there is no variance (so there is no 'SPD problem') 
  but you should still make it deterministic;
- for a Gaussian policy there is the 'SPD problem' except for the logistics
  and for the ones with non-learnable covariance.

Anyway, they can manage low stochasticity and if you want your low-level 
policy to be stochastic you of course can do it.
___________________________________________________________________________

In PFA and RA, while checking for the minimal-norm Pareto-ascent direction, 
the gradients are always normalized (i.e. we normalize the gradients even 
if 'norm(grad_i) < 1'). If the gradients are not normalized, then the 
minimial-norm Pareto-ascent direction is expected to be mostly influenced 
by the elements which have smaller norms. Since gradients with small norm 
are usually associated with objectives that have already achieved a fair 
degree of convergence, the utility of considering these directions for a 
well-balanced correction is questionable. 
This observation points out the necessity of normalizing ALL the gradients 
when looking for the minimal-norm Pareto-ascent direction (Desideri, 
Multiple-gradient descent algorithm for multiobjective optimization, 2012).
___________________________________________________________________________

PFA (usually) needs a randomization of the policy after the optimization of 
one objective. Such randomization is needed to guarantee enough exploration 
to optimize the remaining objectives and depends on the policy used. 

For instance: 
- for a Gibbs policy we can reduce the temperature or scale theta with a 
  constant factor (for instance we can halve theta),
- for a Gaussian policy we can reset the covariance matrix at its initial 
  value or we can, again, scale it.

Using the entropy as a scaling factor seems the most reasonable choice, but 
for Gaussians the (differential) entropy can be negative, while for Gibbs 
policies it could be too low.
For these reasons, the randomization is not integrated in the code and you 
need to choose yours according to the policy and to the MPD.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
